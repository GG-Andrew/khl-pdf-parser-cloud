name: KHL PDF → JSON (cron)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "*/10 * * * *"

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  run:
    runs-on: ubuntu-latest

    env:
      ICS_URL: "https://khl-cloud-results.pages.dev/khl/const_core/data_khl_25-26.ics"  # замени, если у тебя другой .ics
      SEASON: "1369"
      WINDOW_MIN: "10080"   # 7 дней, чтобы точно что-то спарсить
      OUT_DIR: "public/khl/json"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: System deps (tesseract + poppler)
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr tesseract-ocr-rus poppler-utils

      - name: Python 3.11 + deps
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt

      - name: Parse ICS → PDFs → JSON
        run: |
          mkdir -p "$OUT_DIR"
          python - <<'PY'
          # --- упрощённый парсер: тот же, что давал выше ---
          import os, re, json, time, pathlib, asyncio
          import httpx
          from datetime import datetime, timedelta, timezone
          import fitz
          from pdf2image import convert_from_bytes
          import pytesseract

          ICS_URL = os.environ["ICS_URL"]
          SEASON = int(os.environ["SEASON"])
          WINDOW_MIN = int(os.environ["WINDOW_MIN"])
          OUT_DIR = os.environ["OUT_DIR"]

          def norm(t): return re.sub(r"[ \t]+"," ", t.replace("\xa0"," ").replace("\u2009"," ").replace("\u202f"," "))

          RE_TEAMS = re.compile(r'^\s*([A-Za-zА-Яа-яЁё ."«»\-]+)\s[–—\-]\s([A-Za-zА-Яа-яЁё ."«»\-]+)\s*$')
          RE_DATE = re.compile(r"(\d{2}[./-]\d{2}[./-]\d{4})")
          RE_TIME = re.compile(r"(\d{2}:\d{2})")
          RE_REFS = re.compile(r"Главн(?:ые)?\s+судьи?\s*[:\-]?\s*([^\n]+)", re.IGNORECASE)
          RE_LINS = re.compile(r"Линейн(?:ые|ые судьи)\s*[:\-]?\s*([^\n]+)", re.IGNORECASE)
          RE_PL   = re.compile(r"^\s*#?\s*(\d{1,2})\s+([A-Za-zА-Яа-яЁё\-]+)\s+([A-Za-zА-Яа-яЁё\-]+)(?:\s*\((С|Р)\))?\s*(?:\b(С|Р)\b)?", re.IGNORECASE)

          async def fetch(url):
            async with httpx.AsyncClient(follow_redirects=True, timeout=30) as c:
              r = await c.get(url, headers={"Referer":"https://www.khl.ru/"}); r.raise_for_status(); return r.content

          def parse_ics(txt):
            now = datetime.now(timezone.utc)
            out=[]
            for blk in txt.split("BEGIN:VEVENT"):
              mdt = re.search(r"DTSTART(?:;[^:\n]+)?:([0-9TZ]+)", blk)
              if not mdt: continue
              dt = mdt.group(1)
              if dt.endswith("Z"):
                fmt = "%Y%m%dT%H%M%SZ"
                dtstart = datetime.strptime(dt, fmt).replace(tzinfo=timezone.utc)
              else:
                fmt = "%Y%m%dT%H%M%S" if "T" in dt else "%Y%m%d"
                dtstart = datetime.strptime(dt, fmt).replace(tzinfo=timezone.utc)
              if now <= dtstart <= now + timedelta(minutes=WINDOW_MIN):
                m = re.search(r"(\d{6,})", blk)
                if m: out.append((int(m.group(1)), dtstart))
            return out

          def text_from_pdf(b):
            doc = fitz.open(stream=b, filetype="pdf")
            t="\n".join(p.get_text("text") for p in doc); doc.close(); return t

          def ocr_pdf(b):
            from pdf2image import convert_from_bytes
            import pytesseract
            chunks=[]
            for im in convert_from_bytes(b, dpi=320)[:2]:
              chunks.append(pytesseract.image_to_string(im, lang="rus+eng"))
            return "\n".join(chunks)

          def parse_text(full):
            full = norm(full); lines=[norm(l) for l in full.splitlines() if l.strip()]
            teams=[None,None]
            for l in lines[:50]:
              m=RE_TEAMS.search(l); 
              if m: teams=[m.group(1).strip(' "«»'), m.group(2).strip(' "«»')]; break
            date=time_s=None
            for l in lines[:80]:
              if date is None:
                m=RE_DATE.search(l); date=m.group(1).replace("/",".").replace("-",".") if m else None
              if time_s is None:
                m=RE_TIME.search(l); time_s=m.group(1) if m else None
            refs=[]; lins=[]
            m=RE_REFS.search(full);  refs=[x.strip(" .") for x in re.split(r"[;,]", m.group(1)) if x.strip()] if m else []
            m=RE_LINS.search(full);  lins=[x.strip(" .") for x in re.split(r"[;,]", m.group(1)) if x.strip()] if m else []
            # goalies
            goal={"home":[],"away":[]}
            idxs=[i for i,l in enumerate(lines) if re.search(r"\bВратар[ьи]\b", l, re.IGNORECASE)]
            blocks=[]
            for i in idxs:
              blk=[]
              for j in range(i+1, min(i+22,len(lines))):
                t=lines[j].strip()
                if not t or re.match(r"^\w+\s*:", t): break
                blk.append(t)
              blocks.append(blk)
            def parse_blk(b):
              r=[]
              for l in b:
                m=RE_PL.search(l)
                if m:
                  num=int(m.group(1)); last=m.group(2).title(); first=m.group(3).title()
                  stat=(m.group(4) or m.group(5) or "").upper()
                  status={"С":"starter","Р":"reserve"}.get(stat,"")
                  r.append({"number":num,"name":f"{last} {first}","status":status})
              return r
            if blocks: goal["home"]=parse_blk(blocks[0])
            if len(blocks)>1: goal["away"]=parse_blk(blocks[1])
            return {"teams":teams,"date":date,"time_msk":time_s,"main_referees":refs,"linesmen":lins,"goalies":goal}

          async def main():
            ics = (await fetch(ICS_URL)).decode("utf-8","ignore")
            upcoming = parse_ics(ics)
            if not upcoming: 
              print("No matches in window"); return
            import pathlib; pathlib.Path(OUT_DIR).mkdir(parents=True, exist_ok=True)
            async with httpx.AsyncClient(follow_redirects=True, timeout=30) as c:
              for uid,_ in upcoming:
                url=f"https://www.khl.ru/pdf/{SEASON}/{uid}/game-{uid}-start-ru.pdf"
                r=await c.get(url, headers={"Referer":"https://www.khl.ru/"}); 
                if r.status_code!=200: print("skip", uid, r.status_code); continue
                txt = text_from_pdf(r.content)
                if len(txt)<500: txt = ocr_pdf(r.content)
                data = parse_text(txt)
                out = {"ok":True,"uid":uid,"pdf_url":url,"data":data,"ts":int(time.time())}
                (pathlib.Path(OUT_DIR)/f"{uid}.json").write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
                print("Wrote", uid)
          asyncio.run(main())
          PY

      # публикуем public/ в GitHub Pages
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./public

      - name: Deploy to Pages
        uses: actions/deploy-pages@v4
