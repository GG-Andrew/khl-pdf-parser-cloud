name: khl_once_pdftotext

on:
  workflow_dispatch: {}

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  run:
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    env:
      UID: "897694"
      SEASON: "1369"
      OUT_DIR: "public/khl/json"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: MARKER — USING PDFTOTEXT (not fitz)
        run: echo "USING PDFTOTEXT WORKFLOW ✅"

      - name: Install poppler-utils (pdftotext)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y poppler-utils

      - name: Fetch PDF
        run: |
          set -e
          mkdir -p "$OUT_DIR"
          PDF_URL="https://khl.ru/pdf/${{ env.SEASON }}/${{ env.UID }}/game-${{ env.UID }}-start-ru.pdf"
          UA="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
          curl -sSL --http2 --fail \
            -H "Accept: application/pdf,application/octet-stream;q=0.9,*/*;q=0.8" \
            -H "Accept-Language: ru,en;q=0.9" \
            -H "Referer: https://khl.ru/" \
            -H "User-Agent: $UA" \
            -o /tmp/game.pdf "$PDF_URL"

      - name: pdftotext -layout
        run: |
          pdftotext -layout -nopgbrk /tmp/game.pdf /tmp/game.txt
          echo "Dump HEAD/TAIL from pdftotext:"
          python - <<'PY'
          import re, pathlib
          raw = pathlib.Path("/tmp/game.txt").read_text(encoding="utf-8", errors="ignore")
          lines = [re.sub(r"[ \t]+"," ", x).rstrip() for x in raw.splitlines() if x.strip()]
          print("\n===== HEAD 40 =====")
          for i,l in enumerate(lines[:40],1): print(f"{i:03d}: {l}")
          print("\n===== TAIL 80 =====")
          for i,l in enumerate(lines[-80:], max(1,len(lines)-79)): print(f"{i:03d}: {l}")
          PY

      - name: Parse → JSON (teams / time / date / referees / goalies)
        run: |
        echo "PARSER_MARK: V2D"

          python - <<'PY'
          import os, re, json, unicodedata, pathlib
from datetime import datetime

UID = int(os.environ["UID"])
OUT_DIR = os.environ["OUT_DIR"]

# --- helpers ---
def strip_accents(s: str) -> str:
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if unicodedata.category(ch) != "Mn")

def norm_line(s: str) -> str:
    s = strip_accents(s)
    s = s.replace("\xa0"," ").replace("\u2009"," ").replace("\u202f"," ")
    s = re.sub(r"[ \t]+"," ", s)
    return s.strip()

raw = pathlib.Path("/tmp/game.txt").read_text(encoding="utf-8", errors="ignore")
lines = [norm_line(x) for x in raw.splitlines() if x.strip()]
full  = "\n".join(lines)

# --- date/time ---
m_date = re.search(r"(\d{2})[./-](\d{2})[./-](\d{4})", full)
date = f"{m_date.group(1)}.{m_date.group(2)}.{m_date.group(3)}" if m_date else None
m_time = re.search(r"(\d{2}:\d{2})\s*MSK", full, re.I)
time_msk = m_time.group(1) if m_time else None

# --- teams: строка сразу ПОСЛЕ "Матч №..."
teams = [None, None]
try:
    i_match = next(i for i, ln in enumerate(lines) if re.search(r"^Матч\s*№\s*\d+", ln))
    cand = lines[i_match+1] if i_match+1 < len(lines) else ""
    parts = [p.strip(' "«»') for p in re.split(r"\s{2,}", cand) if p.strip()]
    if len(parts) == 2:
        teams = [parts[0], parts[1]]
except StopIteration:
    pass

# --- referees: строка "Главный судья ... Линейный судья ..." + следующая строка с 4 ФИО
main_refs, linesmen = [], []
for i, ln in enumerate(lines):
    if re.search(r"Главный судья.*Линейн\w+ судья", ln, re.I):
        if i + 1 < len(lines):
            names_line = lines[i+1]
            chunks = [c.strip() for c in re.split(r"\s{2,}", names_line) if c.strip()]
            if len(chunks) < 4:
                # fallback: бьём на пары слов
                w = names_line.split()
                chunks = [" ".join(w[j:j+2]) for j in range(0, min(len(w),8), 2)]
            if len(chunks) >= 4:
                main_refs = [chunks[0], chunks[1]]
                linesmen  = [chunks[2], chunks[3]]
        break

# --- goalies: блок "Вратари Вратари" + следующие 1..3 строки, в каждой 2 колонки
goal = {"home": [], "away": []}

def parse_goal_cell(text: str):
    # убираем хвост с ДР и возрастом, если есть
    text = re.sub(r"\s+\d{2}\.\d{2}\.\d{4}\s+\d{1,2}\s*$", "", text)
    m = re.match(r"^\s*(\d{1,2})\s+В\s+(.*)$", text)
    if not m:
        return None
    num = int(m.group(1))
    tail = m.group(2).strip()

    status = ""
    if re.search(r"\bС\b", tail):
        status = "starter"
        tail = re.sub(r"\bС\b", "", tail)
    elif re.search(r"\bР\b", tail):
        status = "reserve"
        tail = re.sub(r"\bР\b", "", tail)

    parts = tail.split()
    if len(parts) >= 2:
        name = f"{parts[0]} {parts[1]}"
    else:
        name = tail
    return {"number": num, "name": name, "status": status}

idx = None
for i, ln in enumerate(lines):
    if re.search(r"\bВратари\b\s+\bВратари\b", ln, re.I):
        idx = i
        break

if idx is not None:
    for row in lines[idx+1 : min(idx+4, len(lines))]:
        cols = [c for c in re.split(r"\s{2,}", row) if c.strip()]
        if len(cols) >= 2:
            L = parse_goal_cell(cols[0])
            R = parse_goal_cell(cols[-1])
            if L: goal["home"].append(L)
            if R: goal["away"].append(R)

out = {
    "ok": True,
    "uid": UID,
    "data": {
        "teams": teams,
        "date": date,
        "time_msk": time_msk,
        "main_referees": main_refs,
        "linesmen": linesmen,
        "goalies": goal
    }
}

p = pathlib.Path(OUT_DIR) / f"{UID}.json"
p.parent.mkdir(parents=True, exist_ok=True)
p.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
print("WROTE_JSON", p)

# --- Публичные дампы на Pages для быстрой диагностики ---
base = pathlib.Path(OUT_DIR).parent / "raw"
base.mkdir(parents=True, exist_ok=True)
(pathlib.Path(base) / f"{UID}_game.txt").write_text(raw, encoding="utf-8")
(pathlib.Path(base) / f"{UID}_lines.json").write_text(json.dumps(lines[:120], ensure_ascii=False, indent=2), encoding="utf-8")


          PY

      - name: Upload artifact (public)
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./public

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
