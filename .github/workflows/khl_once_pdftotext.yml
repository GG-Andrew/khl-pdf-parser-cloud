name: khl_once_pdftotext

on:
  workflow_dispatch: {}

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  run:
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    env:
      UID: "897694"
      SEASON: "1369"
      OUT_DIR: "public/khl/json"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: MARKER — USING PDFTOTEXT (not fitz)
        run: echo "USING PDFTOTEXT WORKFLOW ✅"

      - name: Install poppler-utils (pdftotext)
        run: |
          sudo apt-get update -y
          sudo apt-get install -y poppler-utils

      - name: Fetch PDF
        run: |
          set -e
          mkdir -p "$OUT_DIR"
          PDF_URL="https://khl.ru/pdf/${{ env.SEASON }}/${{ env.UID }}/game-${{ env.UID }}-start-ru.pdf"
          UA="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
          curl -sSL --http2 --fail \
            -H "Accept: application/pdf,application/octet-stream;q=0.9,*/*;q=0.8" \
            -H "Accept-Language: ru,en;q=0.9" \
            -H "Referer: https://khl.ru/" \
            -H "User-Agent: $UA" \
            -o /tmp/game.pdf "$PDF_URL"

      - name: pdftotext -layout
        run: |
          pdftotext -layout -nopgbrk /tmp/game.pdf /tmp/game.txt
          echo "Dump HEAD/TAIL from pdftotext:"
          python - <<'PY'
          import re, pathlib
          raw = pathlib.Path("/tmp/game.txt").read_text(encoding="utf-8", errors="ignore")
          lines = [re.sub(r"[ \t]+"," ", x).rstrip() for x in raw.splitlines() if x.strip()]
          print("\n===== HEAD 40 =====")
          for i,l in enumerate(lines[:40],1): print(f"{i:03d}: {l}")
          print("\n===== TAIL 80 =====")
          for i,l in enumerate(lines[-80:], max(1,len(lines)-79)): print(f"{i:03d}: {l}")
          PY

      - name: Parse → JSON (teams / time / date / referees / goalies)
        run: |
        echo "PARSER_MARK: V2D"

          python - <<'PY'
         import os, re, json, unicodedata, pathlib

UID = int(os.environ["UID"])
OUT_DIR = os.environ["OUT_DIR"]

# ---- read text produced by: pdftotext -layout -nopgbrk /tmp/game.pdf /tmp/game.txt
raw_text = pathlib.Path("/tmp/game.txt").read_text(encoding="utf-8", errors="ignore")

# keep raw lines (НЕ сжимаем пробелы — важна колонночная разметка)
raw_lines = [ln.rstrip("\n\r") for ln in raw_text.splitlines()]

# нормализованная версия (для поиска дат/маркеров), но ОТДЕЛЬНО
def strip_accents(s: str) -> str:
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if unicodedata.category(ch) != "Mn")

def norm_line(s: str) -> str:
    s = s.replace("\xa0"," ").replace("\u2009"," ").replace("\u202f"," ")
    s = re.sub(r"[ \t]+"," ", s).strip()
    return strip_accents(s)

norm_lines = [norm_line(x) for x in raw_lines]

# ---- date / time
full_norm = "\n".join(norm_lines)
m_date = re.search(r"(\d{2})[./-](\d{2})[./-](\d{4})", full_norm)
date = f"{m_date.group(1)}.{m_date.group(2)}.{m_date.group(3)}" if m_date else None

m_time = re.search(r"(\d{2}:\d{2})\s*MSK", full_norm, re.I)
time_msk = m_time.group(1) if m_time else None

# ---- teams: берем СЛЕДУЮЩУЮ строку после "Матч №..."
teams = [None, None]
try:
    i_match = next(i for i, ln in enumerate(norm_lines) if re.search(r"^Матч\s*№\s*\d+", ln))
    raw_after = raw_lines[i_match+1] if i_match+1 < len(raw_lines) else ""
    # ВАЖНО: делим по 2+ пробелам — только на raw!
    parts = [p.strip(' "«»') for p in re.split(r"\s{2,}", raw_after) if p.strip()]
    if len(parts) >= 2:
        teams = [parts[0], parts[-1]]  # на всякий случай берём первое и последнее
except StopIteration:
    pass

# ---- referees: ищем строку с ролями, И берём следующую raw-строку с двумя колонками
main_refs, linesmen = [], []
for i, ln in enumerate(norm_lines):
    if re.search(r"Главный судья.*Линейн\w+ судья", ln, re.I):
        if i + 1 < len(raw_lines):
            raw_names = raw_lines[i+1]
            cols = [c.strip() for c in re.split(r"\s{2,}", raw_names) if c.strip()]
            # ожидаем 4 ФИО: 2 главных + 2 линейных; если больше — берём первые 4
            if len(cols) >= 4:
                main_refs = cols[0:2]
                linesmen  = cols[2:4]
        break

# ---- goalies: ищем "Вратари   Вратари" (в норм — можно, но строки для парсинга берём raw)
goal = {"home": [], "away": []}

def parse_goal_cell(text: str):
    # обрезаем хвосты с ДР и возрастом, если есть
    text = re.sub(r"\s+\d{2}\.\d{2}\.\d{4}\s+\d{1,2}\s*$", "", text)
    # номер, буква В, далее ФИО и метки С/Р
    m = re.match(r"\s*(\d{1,2})\s*В\s+(.*)$", text)
    if not m:
        return None
    num = int(m.group(1))
    tail = m.group(2).strip()

    status = ""
    # статус С/Р может встречаться отдельно
    if re.search(r"\bС\b", tail):
        status = "starter"
        tail = re.sub(r"\bС\b", "", tail)
    elif re.search(r"\bР\b", tail):
        status = "reserve"
        tail = re.sub(r"\bР\b", "", tail)

    # берём первые два токена как Фамилия Имя (минимально рабочее)
    toks = [t for t in tail.split() if t]
    name = " ".join(toks[:2]) if len(toks) >= 2 else tail
    return {"number": num, "name": name, "status": status}

i_vr = None
for i, ln in enumerate(norm_lines):
    if re.search(r"\bВратари\b\s+\bВратари\b", ln, re.I):
        i_vr = i
        break

if i_vr is not None:
    # возьмём 1..3 строки ниже — в них обычно по одному киперу в каждой колонке
    for row in raw_lines[i_vr+1 : min(i_vr+4, len(raw_lines))]:
        cols = [c for c in re.split(r"\s{2,}", row) if c.strip()]
        if len(cols) >= 2:
            L = parse_goal_cell(cols[0])
            R = parse_goal_cell(cols[-1])
            if L: goal["home"].append(L)
            if R: goal["away"].append(R)

# ---- write JSON
out = {
    "ok": True,
    "uid": UID,
    "data": {
        "teams": teams,
        "date": date,
        "time_msk": time_msk,
        "main_referees": main_refs,
        "linesmen": linesmen,
        "goalies": goal,
    },
}
p = pathlib.Path(OUT_DIR) / f"{UID}.json"
p.parent.mkdir(parents=True, exist_ok=True)
p.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")
print("WROTE_JSON", p)

# ---- DIAGNOSTICS for Pages
raw_dir = pathlib.Path(OUT_DIR).parent / "raw"   # public/khl/raw
raw_dir.mkdir(parents=True, exist_ok=True)
(raw_dir / f"{UID}_game.txt").write_text(raw_text, encoding="utf-8")
(raw_dir / f"{UID}_lines.json").write_text(json.dumps(raw_lines[:150], ensure_ascii=False, indent=2), encoding="utf-8")

# показать дерево — увидишь в логах, что реально попадёт в Pages
import subprocess, shlex
subprocess.run(shlex.split("bash -lc 'echo --- TREE public; find public -maxdepth 3 -type f -print | sort'"))


          PY

      - name: Upload artifact (public)
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./public

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
